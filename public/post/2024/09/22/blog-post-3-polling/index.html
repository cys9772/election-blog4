<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Blog Post 3 –– Polling and Prediction | A minimal Hugo website</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Blog Post 3 –– Polling and Prediction</span></h1>
<h2 class="author">Chris S</h2>
<h2 class="date">2024/09/22</h2>
</div>

<main>
<p><em>This blog is an ongoing assignment for Gov 1347: Election Analytics, a course at Harvard College taught by Professor <a href="https://www.ryandenos.com/">Ryan Enos</a>. It will be updated weekly and culminate in a predictive model of the 2024 presidential election.</em></p>
<h2 id="introduction----how-can-we-best-use-polls-to-predict-election-outcomes">Introduction &ndash; How can we best use <strong>polls</strong> to predict <strong>election outcomes?</strong></h2>
<p>What&rsquo;s the first thing you hear when you flick on news channels covering any election? They talk about polls. We&rsquo;ve done hundreds of them ourselves in our lifetime, from restaurant surveys to product reviews and presidential candidates. These quick little quizzes help provide direct insights from the public about various issues, including voting intensions, candidate favoriability, and more. More generally, they serve as feedback loops, allowing parties and campaigns to adjust their strategies based on how the public responds during an election cycle. Breaking these statistics down by demographic categories allow these candidates to further tailer their messaging to specific audiences.</p>
<p>However, there are common pitfalls related to polling, including sampling bias, nonresponse bias, timing, and more. It&rsquo;s our job to figure out what matters and what doesn&rsquo;t. More importantly, can we use them to help predict election outcomes,  or will they simply lead us astray? Let&rsquo;s dive in!</p>
<h2 id="2020-polling----a-detailed-analysis">2020 Polling &ndash; A Detailed Analysis</h2>
<img src="https://example.org/post/2024/09/22/blog-post-3-polling/index_files/figure-html/unnamed-chunk-2-1.png" width="672" />
<p>Immediately, we can notice some interesting trends. First, before COVID_19 hit, both democratic and republican polling averages were roughly stable. However, once COVID-19 was in full swing, we see a significant decline in Republican approval, likely due to the public&rsquo;s perception of how the Trump was handling the pandemic, on both the healthcare and economic fronts. With George Floyd&rsquo;s death, we see a sharp decline in Republican approval, especially when faced with handling racial justice issues and social upheaval. Both the RNC and DNCs showed a bump in approval allowing parties to get in front of viewers, increase media coverage, and consolidating support.</p>
<img src="https://example.org/post/2024/09/22/blog-post-3-polling/index_files/figure-html/unnamed-chunk-3-1.png" width="672" />
<p>Looking at 2024 polling averages, we see a general candidate disapproval trend for both parties. Particularly for the Biden administration, the democratic party was struggling to gain ground as the November election quickly approached. The sharp increase we observe for the democrats has to do with Biden&rsquo;s decision to drop out of the race and endorse Kamala Harris as the democratic presidential candidate. Moreover, her strong performance in the recent debate proved her capabilities as a leader and vision for the future which many American seemed to resonate with.</p>
<p>Altogether, we can see the influence of major political, social, and health-related events on candidate approval, which can be captured by polling averages. Now the question is, do these polling behaviors provide any predictive power. Let&rsquo;s find out!</p>
<h2 id="polls-and-predictions----how-about-november">Polls and Predictions &ndash; How about November?</h2>
<p>Before we look into 2024, let&rsquo;s examine how powerful November polling averages can be at predicting election outcomes for the democratic party in 2020:</p>
<pre><code>## 
## Call:
## lm(formula = pv2p ~ nov_poll, data = subset(d_poll_nov, party == 
##     &quot;DEM&quot;))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.0155 -2.4353 -0.3752  1.4026  5.8014 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  14.2936     7.1693   1.994 0.069416 .  
## nov_poll      0.7856     0.1608   4.885 0.000376 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.968 on 12 degrees of freedom
## Multiple R-squared:  0.6654,	Adjusted R-squared:  0.6375 
## F-statistic: 23.86 on 1 and 12 DF,  p-value: 0.0003756
</code></pre>
<p>There are a few critical results to note about the model above. First, the coefficient for <strong>nov_poll</strong> (0.7856) means that for each 1-point increase in November polling support, the predicted vote share increases by 0.79. Moreover, the p-value is extremely small (0.000376), indicating that the <strong>nov_poll</strong> variable is a statistically significant predictor of pv2p. What&rsquo;s more impressive is the multiple R-squared value of 0.66554 (closer to 1 is better), showing us that the model provides a reasonably good fit and reliability. In the context of the 2020 election results, we can see that polling support in November was a strong indicator of electoral success. With it&rsquo;s proximity to election day, polls can capture the most recent public opinion, particularly since voters will have likely made up their minds by now. As we&rsquo;ve seen before, major campaign events have great influences on voter support and approval, and as the election nears, there are simply less of these so opinions are less volatile.</p>
<p>Instead of focusing solely on the democratic party, let&rsquo;s widen our scope to all parties in the dataset in the following model:</p>
<pre><code>## 
## Call:
## lm(formula = pv2p ~ nov_poll, data = d_poll_nov)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6190 -1.6523 -0.5808  1.3629  6.0220 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 17.92577    4.15543   4.314 0.000205 ***
## nov_poll     0.70787    0.09099   7.780 2.97e-08 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.75 on 26 degrees of freedom
## Multiple R-squared:  0.6995,	Adjusted R-squared:  0.6879 
## F-statistic: 60.52 on 1 and 26 DF,  p-value: 2.974e-08
</code></pre>
<p>Similar to our previous model, we see that the <strong>nov_poll</strong> coefficient (0.70787) means that for each 1-point increase in November polling support, the predicted vote share increases by roughly 0.71 points. The positive relationship indicates that higher polling support in November is strongly associated with higher vote share. Once again, we see that the p-value is extremely lough, reinforcing the statistical significance of November&rsquo;s reliability as an indicator. A strong 0.7 R-squared value also provides confidence that the model is robust and capturing the data effectively. More importantly, this model confirms that November polls are a strong predictor of election outcomes, regardless of party affiliation.</p>
<h2 id="the-power-of-weeks-regularization-methods">The Power of Weeks? Regularization Methods</h2>
<p>Instead of looking just at November results, let&rsquo;s examine how our model performs when we consider weekly polling averages. Of course, we run into an alarming issue &ndash; multicolinearity. This means that we have several variables in our model that are correlated, producing a skewed view of our results. To handle this, we&rsquo;ll use ridge regression, which introduces a <strong>penalty term</strong> to the least squares cost function, preventing the coefficient from becoming too large, helping to adjust the multicolinearity factor. Let&rsquo;s examine our results:</p>
<pre><code>## 
## Call:
## lm(formula = paste0(&quot;pv2p ~ &quot;, paste0(&quot;poll_weeks_left_&quot;, 0:30, 
##     collapse = &quot; + &quot;)), data = d_poll_weeks_train)
## 
## Residuals:
## ALL 28 residuals are 0: no residual degrees of freedom!
## 
## Coefficients: (4 not defined because of singularities)
##                    Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)        28.25534        NaN     NaN      NaN
## poll_weeks_left_0   3.24113        NaN     NaN      NaN
## poll_weeks_left_1   0.02516        NaN     NaN      NaN
## poll_weeks_left_2  -8.87360        NaN     NaN      NaN
## poll_weeks_left_3   7.91455        NaN     NaN      NaN
## poll_weeks_left_4   0.74573        NaN     NaN      NaN
## poll_weeks_left_5   1.41567        NaN     NaN      NaN
## poll_weeks_left_6  -4.58444        NaN     NaN      NaN
## poll_weeks_left_7   4.63361        NaN     NaN      NaN
## poll_weeks_left_8  -0.95121        NaN     NaN      NaN
## poll_weeks_left_9  -1.55307        NaN     NaN      NaN
## poll_weeks_left_10 -1.38062        NaN     NaN      NaN
## poll_weeks_left_11  1.74881        NaN     NaN      NaN
## poll_weeks_left_12 -1.28871        NaN     NaN      NaN
## poll_weeks_left_13 -0.08482        NaN     NaN      NaN
## poll_weeks_left_14  0.87498        NaN     NaN      NaN
## poll_weeks_left_15 -0.16310        NaN     NaN      NaN
## poll_weeks_left_16 -0.34501        NaN     NaN      NaN
## poll_weeks_left_17 -0.38689        NaN     NaN      NaN
## poll_weeks_left_18 -0.06281        NaN     NaN      NaN
## poll_weeks_left_19 -0.17204        NaN     NaN      NaN
## poll_weeks_left_20  1.52230        NaN     NaN      NaN
## poll_weeks_left_21 -0.72487        NaN     NaN      NaN
## poll_weeks_left_22 -2.76531        NaN     NaN      NaN
## poll_weeks_left_23  4.90361        NaN     NaN      NaN
## poll_weeks_left_24 -2.04431        NaN     NaN      NaN
## poll_weeks_left_25 -0.76078        NaN     NaN      NaN
## poll_weeks_left_26 -0.47860        NaN     NaN      NaN
## poll_weeks_left_27       NA         NA      NA       NA
## poll_weeks_left_28       NA         NA      NA       NA
## poll_weeks_left_29       NA         NA      NA       NA
## poll_weeks_left_30       NA         NA      NA       NA
## 
## Residual standard error: NaN on 0 degrees of freedom
## Multiple R-squared:      1,	Adjusted R-squared:    NaN 
## F-statistic:   NaN on 27 and 0 DF,  p-value: NA
</code></pre>
<p>The models shows that we have no residual degrees of freedom, meaning we have perfect multicolinearity, which happens when there is an exact linear dependence among the predicators, in this case, weekly polling averages. The R-squared value is 1.0, indicating perfect fit, but this misleading since this fit is not due to the model&rsquo;s inherent capabilities, but rather from overfitting caused by too many correlated predictors.</p>
<p>To better understand what we&rsquo;re doing, let&rsquo;s visualize our ridge regression:</p>
<img src="https://example.org/post/2024/09/22/blog-post-3-polling/index_files/figure-html/unnamed-chunk-8-1.png" width="672" />
<pre><code>## 32 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                              s1
## (Intercept)        29.951147799
## poll_weeks_left_0   0.032163983
## poll_weeks_left_1   0.025440084
## poll_weeks_left_2   0.024404320
## poll_weeks_left_3   0.024688870
## poll_weeks_left_4   0.024695646
## poll_weeks_left_5   0.024725772
## poll_weeks_left_6   0.024080438
## poll_weeks_left_7   0.023636908
## poll_weeks_left_8   0.024487501
## poll_weeks_left_9   0.026498950
## poll_weeks_left_10  0.025642838
## poll_weeks_left_11  0.021361476
## poll_weeks_left_12  0.017386999
## poll_weeks_left_13  0.013378030
## poll_weeks_left_14  0.010078675
## poll_weeks_left_15  0.007248494
## poll_weeks_left_16  0.012943440
## poll_weeks_left_17  0.012879654
## poll_weeks_left_18  0.011157452
## poll_weeks_left_19  0.008302783
## poll_weeks_left_20  0.004012987
## poll_weeks_left_21  0.003350434
## poll_weeks_left_22  0.004458406
## poll_weeks_left_23  0.001019583
## poll_weeks_left_24 -0.002711193
## poll_weeks_left_25 -0.002447895
## poll_weeks_left_26  0.001121142
## poll_weeks_left_27  0.005975853
## poll_weeks_left_28  0.011623984
## poll_weeks_left_29  0.013833925
## poll_weeks_left_30  0.018964139
</code></pre>
<p>We can clearly see <strong>lambda</strong> acting as the regularization parameter here, adding more penalty to the coefficients and shrinking them towards zero to fight the multicolinearity and overfitting. This prevents any single variable from dominating the model, which is particularly important when dealing with models containing lots of multicolinearity. Of course, there are many other regularization methods, including Lasso, and Elastic Net, each with their own attributes. For the purposes of this blog, we will dive straight into a comparison between all three and draw conclusions, instead of an in-depth examination of each method:</p>
<pre><code>## [1] 9.575001
</code></pre>
<pre><code>## [1] 3.61215
</code></pre>
<pre><code>## [1] 4.302942
</code></pre>
<img src="https://example.org/post/2024/09/22/blog-post-3-polling/index_files/figure-html/unnamed-chunk-9-1.png" width="672" />
<p>By visualizing the four methods above, we see how OLS coefficients are larger compared to the regularized methods, which suggests strong overfitting, particularly in the presence of multicolinearity. Lasso and Ridge perform reasonably well, but <strong>Elastic Net</strong> performs the best, striking a balance between the shrinkage and variable selection, by reducing model complexity and highlight key predictors.</p>
<p>Now that we have our chosen method, let&rsquo;s what the model tells us about the 2024 election results:</p>
<pre><code>## [1]  7 36
</code></pre>
<pre><code>##            s1
## [1,] 51.79268
## [2,] 50.65879
</code></pre>
<p>Our current polling model predicts that Harris will win the results indicate that Harris will win the two-party vote share with roughly a 2% advantage over Trump.</p>
<h2 id="a-battle-of-two-polls">A Battle of Two Polls</h2>
<p>To extend our knowledge of polling, let&rsquo;s take a look at two different approaches. <a href="https://www.natesilver.net/p/model-methodology-2024">Silver&rsquo;s</a> methodology takes on a more conservative approach, by keeping his model largely unchanged from 2020, with a few specific updates. At large, the model continues to rely heavily on historical election data and assumes that voter behavior will remain consistent with a high degree of continuity. This helps reinforce the model&rsquo;s stability and reduces the risk of overfitting to recent events or anomalies.</p>
<p>In contrast, <a href="https://abcnews.go.com/538/538s-2024-presidential-election-forecast-works/story?id=110867585">Morris</a> takes on a far more nuanced view of the 2024 model, completely rebuilding it and incorporating a more holistic integration of polling and fundamentals, such as the economy. His model combines historical voting patterns with demographic data and geographic proximity. His use of fundamental indicators, like GDP growth and employment, and political factors (incumbency, presidential approval ratings) helps support a more dynamic relationship between variables in his model. His use of extensive correlation matrices help project state-level polling movement and adds to the felixibiltiy and responsiveness of the model.</p>
<p>Overall, I prefer <strong>538&rsquo;s model</strong> as it integrates a wide range of indicators to produce a more nuanced and comprehensive understanding of the electoral dynamics at play. Especially considering an election with so many changes and polarization, these fluctuations will be important factors to consider when building a model. This holistic approach to combine fundamentals and historical election data helps to handle uncertainty int he model and provide a more robust prediction of the election. Further, it&rsquo;s ability to update in real-time with new polling and fundamental data allows viewers to observe the most current state of the race. More importantly, he accounts for polling errors and state similarities which can provide a balanced view that adapts to recent trends without extensive overfitting.</p>
<h2 id="data-sources">Data Sources:</h2>
<p>Data are from the GOV 1347 course materials. All files can be found <a href="https://github.com/cys9772/election-blog4">here</a>. All external sources are hyperlinked.</p>

</main>

  <footer>
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<script src="//cdn.jsdelivr.net/combine/npm/katex/dist/katex.min.js,npm/katex/dist/contrib/auto-render.min.js,npm/@xiee/utils/js/render-katex.js" defer></script>

<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/center-img.min.js" defer></script>

  
  <hr/>
  © <a href="https://yihui.org">Yihui Xie</a> 2017 &ndash; 2024 | <a href="https://github.com/yihui">Github</a> | <a href="https://twitter.com/xieyihui">Twitter</a>
  
  </footer>
  </body>
</html>


---
title: Blog Post 3 –– Polling and Prediction
author: Chris S
date: '2024-09-22'
slug: blog-post-3-polling
categories: []
tags: []
---

```{r}
library(ggplot2)
library(maps)
library(knitr)
library(tidyverse)
library(dplyr)
library(broom)
library(car)
library(caret)
library(tidyverse)
library(glmnet)

d_pollav_natl <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/national_polls_1968-2024.csv")
d_pollav_state <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/state_polls_1968-2024.csv")
d_vote <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/popvote_1948-2020.csv")
d_vote$party[d_vote$party == "democrat"] <- "DEM"
d_vote$party[d_vote$party == "republican"] <- "REP"
d_poll_weeks <- d_pollav_natl |> 
  group_by(year, party, weeks_left) |>
  summarize(mean_poll_week = mean(poll_support)) |> 
  filter(weeks_left <= 30) |> 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) |> 
  left_join(d_vote, by = c("year", "party"))
```

*This blog is an ongoing assignment for Gov 1347: Election Analytics, a course at Harvard College taught by Professor [Ryan Enos](https://www.ryandenos.com/). It will be updated weekly and culminate in a predictive model of the 2024 presidential election.*

## Introduction -- How can we best use **polls** to predict **election outcomes?**

What's the first thing you hear when you flick on news channels covering any election? They talk about polls. We've done hundreds of them ourselves in our lifetime, from restaurant surveys to product reviews and presidential candidates. These quick little quizzes help provide direct insights from the public about various issues, including voting intensions, candidate favoriability, and more. More generally, they serve as feedback loops, allowing parties and campaigns to adjust their strategies based on how the public responds during an election cycle. Breaking these statistics down by demographic categories allow these candidates to further tailer their messaging to specific audiences. 

However, there are common pitfalls related to polling, including sampling bias, nonresponse bias, timing, and more. It's our job to figure out what matters and what doesn't. More importantly, can we use them to help predict election outcomes,  or will they simply lead us astray? Let's dive in!

## 2020 Polling -- A Detailed Analysis

```{r}
d_pollav_natl |> 
  filter(year == 2020) |> 
  ggplot(aes(x = poll_date, y = poll_support, color = party)) +
  geom_rect(xmin = as.Date("2020-08-17"), xmax = as.Date("2020-08-20"), ymin = 47.5, ymax = 100, alpha = 0.1, color = NA, fill = "grey") + 
  annotate("text", x = as.Date("2020-08-07"), y = 51.5, label = "DNC", size = 4) +
  geom_rect(xmin = as.Date("2020-08-24"), xmax = as.Date("2020-08-27"), ymin = 0, ymax = 47.2, alpha = 0.1, color = NA, fill = "grey") +
  annotate("text", x = as.Date("2020-09-04"), y = 45, label = "RNC", size = 4) +
  geom_rect(xmin = as.Date("2020-10-02"), xmax = as.Date("2020-10-12"), ymin = 0, ymax = 42.7, alpha = 0.05, color = NA, fill = "grey") +
  
  geom_point(size = 1) + 
  geom_line() + 
  
  geom_segment(x = as.Date("2020-03-12"), xend = as.Date("2020-03-12"), y = 0, yend = 44.8, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-03-12"), y = 42.5, label = "COVID \n Market Crash", size = 3) +
  geom_segment(x = as.Date("2020-04-08"), xend = as.Date("2020-04-08"), y = 49, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-03-25"), y = 51.3, label = "Bernie Ends Run", size = 3) +
  geom_segment(x = as.Date("2020-04-16"), xend = as.Date("2020-04-16"), y = 0, yend = 44, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-04-16"), y = 44.7, label = "22 mil \n Unemployment", size = 3) +
  geom_segment(x = as.Date("2020-05-27"), xend = as.Date("2020-05-27"), y = 0, yend = 43, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-06-05"), y = 44, label = "100k COVID Dead, \n George Floyd", size = 3) +
  
  geom_segment(x = as.Date("2020-07-14"), xend = as.Date("2020-07-14"), y = 0, yend = 50.3, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-06-19"), y = 47.5, label = "Moderna Announces", size = 3) +
  
  geom_segment(x = as.Date("2020-09-29"), xend = as.Date("2020-09-29"), y = 50, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-9-12"), y = 49.5, label = "Pres. Debate", size = 3) +
  geom_segment(x = as.Date("2020-10-07"), xend = as.Date("2020-10-07"), y = 51.7, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-10-17"), y = 50.3, label = "VP Debate", size = 3) +
  geom_segment(x = as.Date("2020-10-22"), xend = as.Date("2020-10-22"), y = 52, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-10-30"), y = 51.5, label = "Pres. Debate", size = 3) +
  annotate("text", x = as.Date("2020-10-15"), y = 43.7, label = "Trump Has COVID", size = 3) +
  geom_segment(x = as.Date("2020-09-18"), xend = as.Date("2020-09-18"), y = 50, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-09-03"), y = 51.5, label = "RBG Passes", size = 3) +
  
  scale_x_date(date_labels = "%b %d") + 
  scale_color_manual(values = c("dodgerblue4", "firebrick1")) +
  labs(x = "Date",
       y = "Average Poll Approval", 
       title = "Polling Averages by Date, 2020 (with Game Changers)") + 
  theme_classic()
```

Immediately, we can notice some interesting trends. First, before COVID_19 hit, both democratic and republican polling averages were roughly stable. However, once COVID-19 was in full swing, we see a significant decline in Republican approval, likely due to the public's perception of how the Trump was handling the pandemic, on both the healthcare and economic fronts. With George Floyd's death, we see a sharp decline in Republican approval, especially when faced with handling racial justice issues and social upheaval. Both the RNC and DNCs showed a bump in approval allowing parties to get in front of viewers, increase media coverage, and consolidating support.

```{r}
d_pollav_natl |> 
  filter(year == 2024) |> 
  ggplot(aes(x = poll_date, y = poll_support, color = party)) +
  geom_point(size = 1) + 
  geom_line() + 
  scale_x_date(date_labels = "%b %d") + 
  scale_color_manual(values = c("dodgerblue4", "firebrick1")) +
  labs(x = "Date",
       y = "Average Poll Approval", 
       title = "Polling Averages by Date, 2024") + 
  theme_classic()
```

Looking at 2024 polling averages, we see a general candidate disapproval trend for both parties. Particularly for the Biden administration, the democratic party was struggling to gain ground as the November election quickly approached. The sharp increase we observe for the democrats has to do with Biden's decision to drop out of the race and endorse Kamala Harris as the democratic presidential candidate. Moreover, her strong performance in the recent debate proved her capabilities as a leader and vision for the future which many American seemed to resonate with.

Altogether, we can see the influence of major political, social, and health-related events on candidate approval, which can be captured by polling averages. Now the question is, do these polling behaviors provide any predictive power. Let's find out!

## Polls and Predictions -- How about November?

Before we look into 2024, let's examine how powerful November polling averages can be at predicting election outcomes for the democratic party in 2020:

```{r}
d_poll_nov <- d_vote |> 
  left_join(d_pollav_natl |> 
              group_by(year, party) |> 
              top_n(1, poll_date) |> 
              select(-candidate), 
            by = c("year", "party")) |> 
  rename(nov_poll = poll_support) |> 
  filter(year <= 2020) |> 
  drop_na()

ols.nov.1 <- lm(pv2p ~ nov_poll, 
                data = subset(d_poll_nov, party == "DEM"))
summary(ols.nov.1)
```

There are a few critical results to note about the model above. First, the coefficient for **nov_poll** (0.7856) means that for each 1-point increase in November polling support, the predicted vote share increases by 0.79. Moreover, the p-value is extremely small (0.000376), indicating that the **nov_poll** variable is a statistically significant predictor of pv2p. What's more impressive is the multiple R-squared value of 0.66554 (closer to 1 is better), showing us that the model provides a reasonably good fit and reliability. In the context of the 2020 election results, we can see that polling support in November was a strong indicator of electoral success. With it's proximity to election day, polls can capture the most recent public opinion, particularly since voters will have likely made up their minds by now. As we've seen before, major campaign events have great influences on voter support and approval, and as the election nears, there are simply less of these so opinions are less volatile.

Instead of focusing solely on the democratic party, let's widen our scope to all parties in the dataset in the following model:

```{r}
ols.nov.2 <- lm(pv2p ~ nov_poll, 
                data = d_poll_nov)
summary(ols.nov.2)
```

Similar to our previous model, we see that the **nov_poll** coefficient (0.70787) means that for each 1-point increase in November polling support, the predicted vote share increases by roughly 0.71 points. The positive relationship indicates that higher polling support in November is strongly associated with higher vote share. Once again, we see that the p-value is extremely lough, reinforcing the statistical significance of November's reliability as an indicator. A strong 0.7 R-squared value also provides confidence that the model is robust and capturing the data effectively. More importantly, this model confirms that November polls are a strong predictor of election outcomes, regardless of party affiliation.

```{r}
d_poll_weeks_train <- d_poll_weeks |> 
  filter(year <= 2020)
d_poll_weeks_test <- d_poll_weeks |> 
  filter(year == 2024)

colnames(d_poll_weeks)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_train)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_test)[3:33] <- paste0("poll_weeks_left_", 0:30)
```

## The Power of Weeks and Ridge Regression

Instead of looking just at November results, let's examine how our model performs when we consider weekly polling averages. Of course, we run into an alarming issue -- multicolinearity. This means that we have several variables in our model that are correlated, producing a skewed view of our results. To handle this, we'll use ridge regression, which introduces a **penalty term** to the least squares cost function, preventing the coefficient from becoming too large, helping to adjust the multicolinearity factor. Let's examine our results:

```{r}
ols.pollweeks <- lm(paste0("pv2p ~ ", paste0( "poll_weeks_left_", 0:30, collapse = " + ")), 
                    data = d_poll_weeks_train)
summary(ols.pollweeks)

x.train <- d_poll_weeks_train |>
  ungroup() |> 
  select(all_of(starts_with("poll_weeks_left_"))) |> 
  as.matrix()
y.train <- d_poll_weeks_train$pv2p
```

The models shows that we have no residual degrees of freedom, meaning we have perfect multicolinearity, which happens when there is an exact linear dependence among the predicators, in this case, weekly polling averages. The R-squared value is 1.0, indicating perfect fit, but this misleading since this fit is not due to the model's inherent capabilities, but rather from overfitting caused by too many correlated predictors.

To better understand what we're doing, let's visualize our ridge regression:

```{r}
ridge.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 0) # Set ridge using alpha = 0. 
plot(ridge.pollsweeks, xvar = "lambda")
coef(ridge.pollsweeks, s = 0.1)
```

We can clearly see **lambda** acting as the regularization parameter here, adding more penalty to the coefficients and shrinking them towards zero to fight the multicolinearity and overfitting. This prevents any single variable from dominating the model, which is particularly important when dealing with models containing lots of multicolinearity. Of course, there are many other regularization methods, including OLS, Lasso, and Elastic Net, each with their own attributes. For the purposes of this blog, we will dive straight into a comparison between all three and draw conclusions, instead of an in-depth examination of each method:

```{r}
lasso.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 1) 
enet.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 0.5)

cv.ridge.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 0)
cv.lasso.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 1)
cv.enet.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 0.5)

# Get minimum lambda values 
lambda.min.ridge <- cv.ridge.pollweeks$lambda.min
lambda.min.lasso <- cv.lasso.pollweeks$lambda.min
lambda.min.enet <- cv.enet.pollweeks$lambda.min

(mse.ridge <- mean((predict(ridge.pollsweeks, s = lambda.min.ridge, newx = x.train) - y.train)^2))
(mse.lasso <- mean((predict(lasso.pollsweeks, s = lambda.min.lasso, newx = x.train) - y.train)^2))
(mse.enet <- mean((predict(enet.pollsweeks, s = lambda.min.enet, newx = x.train) - y.train)^2))

# Generate plot comparing coefficients for each of the weeks. 
d.coefplot <- data.frame("OLS" = coef(ols.pollweeks)[-1], 
                         "Ridge" = coef(ridge.pollsweeks, s = lambda.min.ridge)[-1], 
                         "Lasso" = coef(lasso.pollsweeks, s = lambda.min.lasso)[-1], 
                         "Elastic Net" = coef(enet.pollsweeks, s = lambda.min.enet)[-1]) |> 
  rownames_to_column("coef_name") |> 
  pivot_longer(cols = -coef_name, names_to = "method", values_to = "coef_est") |> 
  mutate(week = rep(0:30, each = 4))

d.coefplot[which(is.na(d.coefplot$coef_est)),]$coef_est <- 0 

d.coefplot |>
  ggplot(aes(x = coef_est, y = reorder(coef_name, -week), color = method)) +
  geom_segment(aes(xend = 0, yend = reorder(coef_name, -week)), alpha = 0.5, lty = "dashed") +
  geom_vline(aes(xintercept = 0), lty = "dashed") +   
  geom_point() + 
  labs(x = "Coefficient Estimate", 
       y = "Coefficient Name", 
       title = "Comparison of Coefficients Across Regularization Methods") + 
  theme_classic()
```

Comparing the fou

### STILL NEED TO TALK ABOUT PREDICTION AND BLOG EXTENSIONS

```{r}
d_pollav_natl |> 
  filter(year == 2024) |> 
  select(weeks_left) |> 
  distinct() |> 
  range() # Let's take week 30 - 7 as predictors since those are the weeks we have polling data for 2024 and historically. 

x.train <- d_poll_weeks_train |>
  ungroup() |> 
  select(all_of(paste0("poll_weeks_left_", 7:30))) |> 
  as.matrix()
y.train <- d_poll_weeks_train$pv2p
x.test <- d_poll_weeks_test |>
  ungroup() |> 
  select(all_of(paste0("poll_weeks_left_", 7:30))) |> 
  as.matrix()

# Using elastic-net for simplicity. 
set.seed(02138)
enet.poll <- cv.glmnet(x = x.train, y = y.train, alpha = 0.5)
lambda.min.enet.poll <- enet.poll$lambda.min

# Predict 2024 national pv2p share using elastic-net. 
(polls.pred <- predict(enet.poll, s = lambda.min.enet.poll, newx = x.test))
```


```{r}
d_econ <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/fred_econ.csv") |> 
  filter(quarter == 2)

# Combine datasets and create vote lags. 
d_combined <- d_econ |> 
  left_join(d_poll_weeks, by = "year") |> 
  filter(year %in% c(unique(d_vote$year), 2024)) |> 
  group_by(party) |> 
  mutate(pv2p_lag1 = lag(pv2p, 1), 
         pv2p_lag2 = lag(pv2p, 2)) |> 
  ungroup() |> 
  mutate(gdp_growth_x_incumbent = GDP_growth_quarterly * incumbent, 
         rdpi_growth_quarterly = RDPI_growth_quarterly * incumbent,
         cpi_x_incumbent = CPI * incumbent,
         unemployment_x_incumbent = unemployment * incumbent,
         sp500_x_incumbent = sp500_close * incumbent) # Generate interaction effects.

# Create fundamentals-only dataset and split into training and test sets. 
d_fund <- d_combined |> 
  select("year", "pv2p", "GDP", "GDP_growth_quarterly", "RDPI", "RDPI_growth_quarterly", "CPI", "unemployment", "sp500_close",
         "incumbent", "gdp_growth_x_incumbent", "rdpi_growth_quarterly", "cpi_x_incumbent", "unemployment_x_incumbent", "sp500_x_incumbent", 
         "pv2p_lag1", "pv2p_lag2") 
x.train.fund <- d_fund |> 
  filter(year <= 2020) |>
  select(-c(year, pv2p)) |> 
  slice(-c(1:9)) |> 
  as.matrix()
y.train.fund <- d_fund |> 
  filter(year <= 2020) |> 
  select(pv2p) |> 
  slice(-c(1:9)) |> 
  as.matrix()
x.test.fund <- d_fund |> 
  filter(year == 2024) |> 
  select(-c(year, pv2p)) |> 
  drop_na() |> 
  as.matrix()
```
```{r}
set.seed(02138)
enet.fund <- cv.glmnet(x = x.train.fund, y = y.train.fund, intercept = FALSE, alpha = 0.5)
lambda.min.enet.fund <- enet.fund$lambda.min

# Predict 2024 national pv2p share using elastic-net. 
(fund.pred <- predict(enet.fund, s = lambda.min.enet.fund, newx = x.test.fund))
  
# Sequester data for combined model.
d_combo <- d_combined |> 
  select("year", "pv2p", "GDP", "GDP_growth_quarterly", "RDPI", "RDPI_growth_quarterly", "CPI", "unemployment", "sp500_close",
         "incumbent", "gdp_growth_x_incumbent", "rdpi_growth_quarterly", "cpi_x_incumbent", "unemployment_x_incumbent", "sp500_x_incumbent", 
         "pv2p_lag1", "pv2p_lag2", all_of(paste0("poll_weeks_left_", 7:30))) 

x.train.combined <- d_combo |> 
  filter(year <= 2020) |> 
  select(-c(year, pv2p)) |> 
  slice(-c(1:9)) |> 
  as.matrix()
y.train.combined <- d_combo |>
  filter(year <= 2020) |> 
  select(pv2p) |> 
  slice(-c(1:9)) |> 
  as.matrix()
x.test.combined <- d_combo |>
  filter(year == 2024) |> 
  select(-c(year, pv2p)) |> 
  drop_na() |> 
  as.matrix()
  
# Estimate combined model.
set.seed(02138)
enet.combined <- cv.glmnet(x = x.train.combined, y = y.train.combined, intercept = FALSE, alpha = 0.5)
lambda.min.enet.combined <- enet.combined$lambda.min

# Predict 2024 national pv2p share using elastic-net.
(combo.pred <- predict(enet.combined, s = lambda.min.enet.combined, newx = x.test.combined))
```
## Data Sources:

Data are from the GOV 1347 course materials. All files can be found [here](https://github.com/cys9772/election-blog4). All external sources are hyperlinked.

---
title: Blog Post 5 -- Demographics, Turnout, and Forests?
author: Chris S
date: '2024-10-06'
slug: blog-post-5-demographics-and-turnout
categories: []
tags: []
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(car)
library(caret)
library(CVXR)
library(foreign)
library(glmnet)
library(haven)
library(janitor)
library(kableExtra)
library(maps)
library(mlr3)
library(randomForest)
library(ranger)
library(RColorBrewer)
library(sf)
library(tidyverse)
library(viridis)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
d_popvote <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/popvote_1948-2020.csv")
d_state_popvote <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/state_popvote_1948_2020.csv")

# Read elector distribution dataset. 
d_ec <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/corrected_ec_1948_2024.csv")

# Read and merge demographics data. 
d_demos <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/demographics.csv")[,-1]

# Read primary turnout data. 
d_turnout <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/turnout_1789_2020.csv")
d_state_turnout <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/state_turnout_1980_2022.csv")
d_state_turnout <- d_state_turnout |> 
  mutate(vep_turnout = as.numeric(str_remove(vep_turnout, "%"))/100) |> 
  select(year, state, vep_turnout)

# Read polling data. 
d_polls <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/national_polls_1968-2024.csv")
d_state_polls <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/state_polls_1968-2024.csv")

# Read and merge 1% voterfile data into one dataset. 
voterfile.sample.files <- list.files("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/state_1pc_samples_aug24")

# Florida example. 
vf_fl <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/state_1pc_samples_aug24/FL_sample.csv")
d_vote <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/popvote_1948-2020.csv")
d_vote$party[d_vote$party == "democrat"] <- "DEM"
d_vote$party[d_vote$party == "republican"] <- "REP"
d_pollav_natl <- read_csv("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/national_polls_1968-2024.csv")

d_poll_weeks <- d_pollav_natl |> 
  group_by(year, party, weeks_left) |>
  summarize(mean_poll_week = mean(poll_support)) |> 
  filter(weeks_left <= 30) |> 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) |> 
  left_join(d_vote, by = c("year", "party"))

d_poll_weeks_train <- d_poll_weeks |> 
  filter(year <= 2020)
d_poll_weeks_test <- d_poll_weeks |> 
  filter(year == 2024)

colnames(d_poll_weeks)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_train)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_test)[3:33] <- paste0("poll_weeks_left_", 0:30)

# Process state-level polling data. 
d_pollav_state <- d_state_polls |> 
  group_by(year, state, party) |>
  mutate(mean_pollav = mean(poll_support, na.rm = TRUE)) |>
  top_n(1, poll_date) |> 
  rename(latest_pollav = poll_support) |>
  select(-c(weeks_left, days_left, poll_date, candidate, before_convention)) |>
  pivot_wider(names_from = party, values_from = c(latest_pollav, mean_pollav))


```


```{r, echo = FALSE, message = FALSE, warning = FALSE}
anes <- read_dta("/Users/chrisshen/Downloads/GOV1347/Election Blog 4/anes_timeseries_cdf_stata_20220916.dta") # Total ANES Cumulative Data File. 

anes <- anes |> 
  mutate(year = VCF0004,
         pres_vote = case_when(VCF0704a == 1 ~ 1, 
                               VCF0704a == 2 ~ 2, 
                               .default = NA), 
         # Demographics
         age = VCF0101, 
         gender = VCF0104, # 1 = Male; 2 = Female; 3 = Other
         race = VCF0105b, # 1 = White non-Hispanic; 2 = Black non-Hispanic, 3 == Hispanic; 4 = Other or multiple races, non-Hispanic; 9 = missing/DK
         educ = VCF0110, # 0 = DK; 1 = Less than high school; 2. High school; 3 = Some college; 4 = College+ 
         income = VCF0114, # 1 = 0-16 percentile; 2 = 17-33 percentile; 3 = 34-67; 4 = 68 to 95; 5 = 96 to 100. 
         religion = VCF0128, # 0 = DK; 1 = Protestant; 2 = Catholic; 3 = Jewish; 4 = Other
         attend_church = case_when(
           VCF0004 < 1972 ~ as.double(as.character(VCF0131)),
           TRUE ~ as.double(as.character(VCF0130))
         ), # 1 = every week - regularly; 2 = almost every week - often; 3 = once or twice a month; 4 = a few times a year - seldom; 5 = never ; 6 = no religious preference
         southern = VCF0113,
         region = VCF0113, 
         work_status = VCF0118,
         homeowner = VCF0146, 
         married = VCF0147,
        
         # 7-point PID
         pid7 = VCF0301, # 0 = DK; 1 = Strong Democrat; 2 = Weak Democrat; 3 = Independent - Democrat; 4 = Independent - Independent; 5 = Independent - Republican; 6 = Weak Republican; 7 = Strong Republican
         
         # 3-point PID
         pid3 = VCF0303, # 0 = DK; 1 = Democrats; 2 = Independents; 3 = Republicans. 
         
         # 3-point ideology. 
         ideo = VCF0804 # 0, 9 = DK; 1 = Liberal; 2 = Moderate; 3 = Conservative
         ) |> 
  select(year, pres_vote, age, gender, race, educ, income, religion, attend_church, southern, 
         region, work_status, homeowner, married, pid7, pid3, ideo)
```

*This blog is an ongoing assignment for Gov 1347: Election Analytics, a course at Harvard College taught by Professor [Ryan Enos](https://www.ryandenos.com/). It will be updated weekly and culminate in a predictive model of the 2024 presidential election.*

## Introduction -- What are connections, if any between demographics and voter turnout? Can demographics and turnout be used to predict election outcomes? If so, how and how well?

As kids, our family, community, experiences, and upbringing all play a role in determining who we become as adults. The way we think, the choices we make, and even the things we care about are often rooted in childhood influences. Can the same be said about voting behavior? Wouldn't it make sense that our demographics -- age, race, income, education, etc -- play roles in how we view political candidates?

For example, a person raised from a working-class background might prioritize economic policies regarding job security and unions. At the same time, someone from an affluent neighborhood may be more concerned about taxes. As the impact of these differences compounds over many years, they influence not only our voting behavior but also whether or not we even show up to vote in the first place.

The question is -- how straightforward is this connection between demographics and voter turnout? What are the nuances within the numbers that we should pay attention to? As always, we'll start with some data.

## 2016 -- A Year of Particular Interest

While every election is. important, some stand out more than others due to social, economic, and political divides. 2016 was a sharp turning point for American politics as voters and candidates wrestled with questions about the gender gap, increased racial divide, shifting education, and more. 

Therefore, let's start with a logistic regression that predicts the likelihood of a voter casting a ballot for a Democrat or Republican based on factors including age, gender, race, etc: 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
anes_year <- anes[anes$year == 2016,] |> 
  select(-c(year, pid7, pid3, ideo)) |>
  mutate(pres_vote = factor(pres_vote, levels = c(1, 2), labels = c("Democrat", "Republican"))) |> 
  filter(!is.na(pres_vote)) |>
  clean_names()

n_features <- length(setdiff(names(anes_year), "pres_vote"))

set.seed(02138)
train.ind <- createDataPartition(anes_year$pres_vote, p = 0.8, list = FALSE)

anes_train <- anes_year[train.ind,]
anes_test <- anes_year[-train.ind,]
logit_fit <- glm(pres_vote ~ ., 
                 family = "binomial", 
                 data = anes_train)

# In-sample goodness-of-fit. 
summary(logit_fit)
```

Let's dissect and interpret some of the important coefficients:

The large positive intercept of roughly 4.358 represents the baseline log-odds of voting Republican when all other variables are held constant is quite strong.

The tiny age estimate of 0.0007 and p-value of 0.804 means that while the coefficient is positive, age has no meaningful impact on the likelihood of voting Republican based on the model. Historically, older voters tend to lean right, while younger generations seem to shift to the left, but the model seems to find this factor to be negligible.

The negative gender estimate of -0.429 and p-value of less than 0.001 suggests that women are less likely than men to vote Republican, which makes sense given historical trends in election data. Particularly for 2016 exit polls, the research found that women primarily favored Hillary Clinton, especially those of minority backgrounds.

The strong negative race estimate of -0.548 and p-value of 0.001 suggest that non-white voters are significantly less likely to vote Republican. This result is unsurprising as white voters have been historically more likely to vote Republican while Black, Hispanic, and other minority voters tend to lean left based on various social and economic policies. The results of the 2016 election confirmed this as Trump won the majority of white voters, but Clinton received overwhelming support from African American, Latino, and Asian voters.

In our lecture, we discussed how education can often be a significant predictor of voting behavior. In this model, we find that the coefficient is negative at -0.340, indicating that more educated individuals are less likely to vote Republican. In the context of 2016, many white college-educated women tended to lean Democratic, and as the divide continues to grow over time, the Democrats have continued to gain strength in this demographic subsection. In general, voters with greater education are exposed to more complex and nuanced discussions of political campaigns, making it harder for simplified and biased propaganda messages to penetrate voter behavior.

Surprisingly, the southern coefficient is negative and significant at -0.410, meaning that voters from the South are less likely to vote Republican according to this model. Historically, the South has remained a key stronghold for Republicans since the 1960s due to initiatives like the Southern Strategy, but changes in demographic shifts or other factors seem to be moving the needle in the other direction.

Overall, the difference between the null deviance and residual deviance shows that improvements were brought about by adding new predictors, meaning that it explains some of the variance in the data but there is still lots of room for improvement.

We can test this further by looking at in-sample accuracy:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# In-sample accuracy.
logit.is <- factor(ifelse(predict(logit_fit, type = "response") > 0.5, 2, 1), 
                   levels = c(1, 2), labels = c("Democrat", "Republican"))
(cm.rf.logit.is <- confusionMatrix(logit.is, anes_train$pres_vote))

```

From the above, we find that the overall accuracy was roughly 0.6734 or 67.34%, presenting the proportion of total predictions (both Democrat and Republican) that were correct. Comparing it to the no Information Rate (NIR) of 52.3%, we can see that our model was significantly better than the NIR, as shown by the tiny p-value. Looking at the balanced accuracy (average of sensitivity and specificity) level of 0.6729, we see that it is similar to the overall accuracy, meaning that the model performs moderately well in both classes (Democrat and republican). 

However, there is still lots of room for improvement, and considering accuracy alone can be misleading when thinking about sampling biases and imbalanced datasets. We may be looking at opportunities to increase our model complexity, such as random forests or gradient boosting to capture more non-linear interactions between demographics and voting behavior.

Before we do that, let's take a look now at the out-of-sample accuracy:

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# Out-of-sample accuracy. 
logit_pred <- factor(ifelse(predict(logit_fit, anes_test, type = "response") > 0.5, 2, 1), 
                     levels = c(1, 2), labels = c("Democrat", "Republican"))
(cm.rf.logit.oos <- confusionMatrix(logit_pred, anes_test$pres_vote))
```

Compared to the previous in-sample accuracy, we see that the out-of-sample accuracy remains consistent, indicating that the model generalizes well to unseen data and isn't overfitting to the training set. For example, if we had seen the in-sample accuracy be much higher than the above, that could suggest significant overfitting. Both moderate Kappa values of around 0.34-0.35 show that the model's level of agreement with the actual outcomes is pretty consistent. 

While there are certainly areas for improvement, altogether, this shows signs of a robust model and reveals that the demographic variables included are fairly stable predictors of voting behavior across different samples of voters.

## Don't Miss the Forest for the Trees

As mentioned before, the logistic regression model is quite simple compared to the arsenal of other models at our disposal. One in particular that catches our attention is the random forest model, which can be used for both regression and classification tasks due to its versatility.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# RANDOM FOREST: 
rf_fit <- ranger(pres_vote ~ ., 
                 mtry = floor(n_features/3), 
                 respect.unordered.factors = "order", 
                 seed <- 02138,
                 classification = TRUE,
                 data = anes_train)

(cm.rf.is <- confusionMatrix(rf_fit$predictions, anes_train$pres_vote))
```

Immediately, we can see that strong accuracy of 0.7031, with a 95% confidence interval of 0.683 to 0.7226, suggesting a fairly robust and confident estimate of the model's performance. Compared to the No Information Rate (NIR), we see that our model performs significantly better than random guessing or simply predicting the majority class. Once again, the balanced accuracy here represents the average of sensitivity and specificity. With a value of 0.7028, we can see that it is able to better predict for both Democrats and Republicans compared to our previous logistic regression model. 

Of course, we have some concerns about overfitting because these results are based on in-sample data. Let's see how the model performs on unseen data and whether or not our results remain consistent:

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# Out-of-sample accuracy. 
rf_pred <- predict(rf_fit, data = anes_test)
(cm.rf.oos <- confusionMatrix(rf_pred$predictions, anes_test$pres_vote))
```

Like before, we see that our out-of-sample accuracy is very close to the in-sample accuracy, showing that the model generalizes well to unseen data and isn't drastically overfitting to maintain performance. Thus, we can say that the model seems to be inherently stronger than the logistic regression and better at capturing the complex interactions between education, race, religion, etc. This is particularly useful for a classification model in elections, whereby misclassifying voters from one party shouldn't be systematically worse than for the other.

## Simulations

To understand more about our model performance, we'll conduct a series of bootstrapped simulations on different randomly sampled splits of the dataset to examine the stability and variability of the models. Let's take a look at the logistic regression model first:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Simulations for Logistic Regression
set.seed(02138)
n_sims <- 1000
sim_logit_results <- replicate(n_sims, {
  train_ind <- createDataPartition(anes_year$pres_vote, p = 0.8, list = FALSE)
  anes_train_sim <- anes_year[train_ind,]
  anes_test_sim <- anes_year[-train_ind,]
  
  logit_sim <- glm(pres_vote ~ ., family = "binomial", data = anes_train_sim)
  predicted_probs <- predict(logit_sim, anes_test_sim, type = "response")
  
  # Predict based on probability threshold (0.5)
  predicted_class <- ifelse(predicted_probs > 0.5, "Republican", "Democrat")
  
  # Return confusion matrix or prediction accuracy
  mean(predicted_class == anes_test_sim$pres_vote)
})

# Summary of results
summary(sim_logit_results)
hist(sim_logit_results, main = "Simulation Distribution - Logistic Regression", xlab = "Accuracy")

```

From the histogram above, we see that the accuracy tends to hover around 66%, with relatively tight distribution results, indicating that the model is stable and performs consistently. This further adds confidence that it will generalize reasonably well when applied to new data, though there is room for improvement since the 66% accuracy still leaves quite some room for prediction error.

Now let's repeat a similar test for the random forest model:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Simulations for Random Forest
set.seed(02138)
n_sims <- 500
sim_rf_results <- replicate(n_sims, {
  train_ind <- createDataPartition(anes_year$pres_vote, p = 0.8, list = FALSE)
  anes_train_sim <- anes_year[train_ind,]
  anes_test_sim <- anes_year[-train_ind,]
  
  rf_fit_sim <- ranger(pres_vote ~ ., 
                       mtry = floor(n_features/3), 
                       respect.unordered.factors = "order", 
                       classification = TRUE,
                       data = anes_train_sim)
  
  rf_pred_sim <- predict(rf_fit_sim, data = anes_test_sim)
  predicted_class <- rf_pred_sim$predictions
  
  # Return confusion matrix or prediction accuracy
  mean(predicted_class == anes_test_sim$pres_vote)
})

# Summary of results
summary(sim_rf_results)
hist(sim_rf_results, main = "Simulation Distribution - Random Forest", xlab = "Accuracy")

```

By running multiple simulations, we can assess whether the random forest model performs consistently or whether its accuracy fluctuates based on the data split. We see that the most frequent accuracy is around 0.70, with a moderate spread, meaning that there is some variability in the model's accuracy depending on the specific dataset, but remains relatively stable.

## Forests and Polling?

In terms of accuracy, the random forest model seems to capture the data most consistently across many of the different models we've explored so far. What if instead of adjusting our predictor variables, we adapted random forest to a new context -- in this case, polling averages? In Blog Post 3, we had the opportunity to look into Elastic-Net and what its predictions are for the 2024 election. 

The results yielded that **Harris will win the vote share by a very slim margin**:

```{r, echo = FALSE, message = FALSE, warning = FALSE}


invisible(d_pollav_natl |> 
  filter(year == 2024) |> 
  select(weeks_left) |> 
  distinct() |> 
  range()) # Let's take week 30 - 7 as predictors since those are the weeks we have polling data for 2024 and historically. 

x.train <- d_poll_weeks_train |>
  ungroup() |> 
  select(all_of(paste0("poll_weeks_left_", 7:30))) |> 
  as.matrix()

y.train <- d_poll_weeks_train$pv2p

x.test <- d_poll_weeks_test |>
  ungroup() |> 
  select(all_of(paste0("poll_weeks_left_", 7:30))) |> 
  as.matrix()

# Using elastic-net for simplicity. 
set.seed(02138)
enet.poll <- cv.glmnet(x = x.train, y = y.train, alpha = 0.5)
lambda.min.enet.poll <- enet.poll$lambda.min

# Predict 2024 national pv2p share using elastic-net. 
(polls.pred <- predict(enet.poll, s = lambda.min.enet.poll, newx = x.test))
```

Now, let's see what random forest says based on the same training data:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Set seed for reproducibility
set.seed(02138)

# Extract training data (weeks left from 7 to 30) as features (predictors)
x.train_rf <- d_poll_weeks_train |>
  ungroup() |> 
  select(all_of(paste0("poll_weeks_left_", 7:30)))

# Extract target variable (pv2p)
y.train_rf <- d_poll_weeks_train$pv2p

# Extract test data (weeks left from 7 to 30) for 2024
x.test_rf <- d_poll_weeks_test |>
  ungroup() |> 
  select(all_of(paste0("poll_weeks_left_", 7:30)))

# Train the random forest model
rf_fit_2024 <- ranger(pv2p ~ ., 
                      data = data.frame(x.train_rf, pv2p = y.train_rf), 
                      mtry = floor(ncol(x.train_rf) / 3), 
                      respect.unordered.factors = "order", 
                      seed = 02138, 
                      classification = FALSE)

# Predict the 2024 vote share using the trained random forest model
rf_pred_2024 <- predict(rf_fit_2024, data = x.test_rf)

# Display the predicted vote shares
rf_pred_2024$predictions

```

We can immediately see that the random forest model predicts a larger gap in vote share compared to the more linear elastic-net model. The ability of the random forest to capture complex interactions gives it an edge in terms of prediction and fit. In the context of polling, sudden shifts or specific weeks of high volatility might be more important for election outcome than what a more linear relationship might suggest.

Regardless, we see that the results predict a very tight race and reinforce the idea that even small events, such as last-minute campaign strategies, swapped candidates, or shifts in voter turnout could drastically affect outcomes.

Either way, our analysis here has provided greater context for what models to use and reveals valuable insights for how we can improve our models down the road...a road only 28 days long! Stay tuned for more!

## Data Sources:

Data are from the GOV 1347 course materials. All files can be found [here](https://github.com/cys9772/election-blog4). All external sources are hyperlinked.
